{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"DataDisAggregation Package","text":"<p>DataDisAggregation is a python package that lets you perform weighted aggregations and weighted disaggregations of data.</p> <p>Development was funded as part of the IND-E project.</p> <p></p>"},{"location":"#install","title":"Install","text":"<pre><code>pip install data-disaggregation\n</code></pre>"},{"location":"01-examples/","title":"Examples","text":"In\u00a0[17]: Copied! <pre>from data_disaggregation.actions import transform\nfrom data_disaggregation.vtypes import VT_Nominal, VT_NumericExt, VT_Numeric, VT_Ordinal\n</pre> from data_disaggregation.actions import transform from data_disaggregation.vtypes import VT_Nominal, VT_NumericExt, VT_Numeric, VT_Ordinal In\u00a0[18]: Copied! <pre># create data (key-values)\ndata = {\n    \"a\": 5,\n    \"b\": 10,\n    \"c\": 30\n}\n\n# create weight mapping\nweights = {    \n    (\"b\", \"D\"): 0.7,\n    (\"c\", \"E\"): 0.7,\n    (\"a\", \"F\"): 0.4,\n    (\"b\", \"F\"): 0.3,\n    (\"c\", \"F\"): 0.3,\n}\n</pre> # create data (key-values) data = {     \"a\": 5,     \"b\": 10,     \"c\": 30 }  # create weight mapping weights = {         (\"b\", \"D\"): 0.7,     (\"c\", \"E\"): 0.7,     (\"a\", \"F\"): 0.4,     (\"b\", \"F\"): 0.3,     (\"c\", \"F\"): 0.3, } In\u00a0[19]: Copied! <pre># if data as categorical/nominal:\n# \"F\" is 5 because \"a\" has largest share 0.4 (mode)\ntransform(VT_Nominal, data, weights)\n</pre> # if data as categorical/nominal: # \"F\" is 5 because \"a\" has largest share 0.4 (mode) transform(VT_Nominal, data, weights) Out[19]: <pre>{'F': 5, 'E': 30, 'D': 10}</pre> In\u00a0[20]: Copied! <pre># if data as categorical/ordinal:\n# \"F\" is 10 because \"b\" has cumulative share (sum of shares &lt;= \"b\") 0.4 + 0.3 = 0.7 \n# that is covering midpoint 0.5\ntransform(VT_Ordinal, data, weights)\n</pre> # if data as categorical/ordinal: # \"F\" is 10 because \"b\" has cumulative share (sum of shares &lt;= \"b\") 0.4 + 0.3 = 0.7  # that is covering midpoint 0.5 transform(VT_Ordinal, data, weights) Out[20]: <pre>{'F': 10, 'E': 30, 'D': 10}</pre> In\u00a0[21]: Copied! <pre># if data is numerical/intensive\n# average (weighted) density\n# D = 10 * 0.7 / 0.7\n# E = 30 * 0.7 / 0.7\n# F = (10 * 0.3 + 30 * 0.3 + 5 * 0.4) / 1.0\ntransform(VT_Numeric, data, weights)\n</pre> # if data is numerical/intensive # average (weighted) density # D = 10 * 0.7 / 0.7 # E = 30 * 0.7 / 0.7 # F = (10 * 0.3 + 30 * 0.3 + 5 * 0.4) / 1.0 transform(VT_Numeric, data, weights) Out[21]: <pre>{'F': 14.0, 'E': 30.0, 'D': 10.0}</pre> In\u00a0[22]: Copied! <pre># if data is numerical/extensive\n# redistribute total sum of 45 according to (relative) shares:\n# D = 10 * 0.7 / 1.0\n# E = 30 * 0.7 / 1.0\n# F = 10 * (0.3 / 1.0) + 30 * (0.3 / 1.0) + 5 * (0.4 / 0.4)\ntransform(VT_NumericExt, data, weights)\n</pre> # if data is numerical/extensive # redistribute total sum of 45 according to (relative) shares: # D = 10 * 0.7 / 1.0 # E = 30 * 0.7 / 1.0 # F = 10 * (0.3 / 1.0) + 30 * (0.3 / 1.0) + 5 * (0.4 / 0.4) transform(VT_NumericExt, data, weights) Out[22]: <pre>{'F': 17.0, 'E': 21.0, 'D': 7.0}</pre> In\u00a0[23]: Copied! <pre>import pandas as pd\nfrom data_disaggregation.actions import transform_pandas\nfrom data_disaggregation.vtypes import VT_Nominal, VT_NumericExt, VT_Numeric, VT_Ordinal\n</pre> import pandas as pd from data_disaggregation.actions import transform_pandas from data_disaggregation.vtypes import VT_Nominal, VT_NumericExt, VT_Numeric, VT_Ordinal In\u00a0[24]: Copied! <pre># data as pandas series (with named index)\nds_data = pd.Series(data).rename_axis(index=\"dim_from\")\nds_data\n</pre> # data as pandas series (with named index) ds_data = pd.Series(data).rename_axis(index=\"dim_from\") ds_data Out[24]: <pre>dim_from\na     5\nb    10\nc    30\ndtype: int64</pre> In\u00a0[25]: Copied! <pre># weights as pandas series (with named indices)\nds_weights = pd.Series(weights).rename_axis(index=[\"dim_from\", \"dim_to\"])\nds_weights\n</pre> # weights as pandas series (with named indices) ds_weights = pd.Series(weights).rename_axis(index=[\"dim_from\", \"dim_to\"]) ds_weights Out[25]: <pre>dim_from  dim_to\nb         D         0.7\nc         E         0.7\na         F         0.4\nb         F         0.3\nc         F         0.3\ndtype: float64</pre> In\u00a0[26]: Copied! <pre># if data as categorical/nominal:\n# \"F\" is 5 because \"a\" has largest share 0.4 (mode)\ntransform_pandas(VT_Nominal, ds_data, ds_weights)\n</pre> # if data as categorical/nominal: # \"F\" is 5 because \"a\" has largest share 0.4 (mode) transform_pandas(VT_Nominal, ds_data, ds_weights) Out[26]: <pre>dim_to\nD         10\nE         30\nF          5\nName: 0, dtype: int64</pre> In\u00a0[27]: Copied! <pre># if data as categorical/ordinal:\n# \"F\" is 10 because \"b\" has cumulative share (sum of shares &lt;= \"b\") 0.4 + 0.3 = 0.7 \n# that is covering midpoint 0.5\ntransform_pandas(VT_Ordinal, ds_data, ds_weights)\n</pre> # if data as categorical/ordinal: # \"F\" is 10 because \"b\" has cumulative share (sum of shares &lt;= \"b\") 0.4 + 0.3 = 0.7  # that is covering midpoint 0.5 transform_pandas(VT_Ordinal, ds_data, ds_weights) Out[27]: <pre>dim_to\nD         10\nE         30\nF         10\nName: 0, dtype: int64</pre> In\u00a0[28]: Copied! <pre># if data is numerical/intensive\n# average (weighted) density\n# D = 10 * 0.7 / 0.7\n# E = 30 * 0.7 / 0.7\n# F = (10 * 0.3 + 30 * 0.3 + 5 * 0.4) / 1.0\ntransform_pandas(VT_Numeric, ds_data, ds_weights)\n</pre> # if data is numerical/intensive # average (weighted) density # D = 10 * 0.7 / 0.7 # E = 30 * 0.7 / 0.7 # F = (10 * 0.3 + 30 * 0.3 + 5 * 0.4) / 1.0 transform_pandas(VT_Numeric, ds_data, ds_weights) Out[28]: <pre>dim_to\nD         10.0\nE         30.0\nF         14.0\nName: 0, dtype: float64</pre> In\u00a0[29]: Copied! <pre># if data is numerical/extensive\n# redistribute total sum of 45 according to (relative) shares:\n# D = 10 * 0.7 / 1.0\n# E = 30 * 0.7 / 1.0\n# F = 10 * (0.3 / 1.0) + 30 * (0.3 / 1.0) + 5 * (0.4 / 0.4)\ntransform_pandas(VT_NumericExt, ds_data, ds_weights)\n</pre> # if data is numerical/extensive # redistribute total sum of 45 according to (relative) shares: # D = 10 * 0.7 / 1.0 # E = 30 * 0.7 / 1.0 # F = 10 * (0.3 / 1.0) + 30 * (0.3 / 1.0) + 5 * (0.4 / 0.4) transform_pandas(VT_NumericExt, ds_data, ds_weights) Out[29]: <pre>dim_to\nD          7.0\nE         21.0\nF         17.0\nName: 0, dtype: float64</pre> In\u00a0[30]: Copied! <pre>idx_out = ds_weights.index\ntransform_pandas(VT_NumericExt, ds_data, ds_weights, dim_out=idx_out)\n</pre> idx_out = ds_weights.index transform_pandas(VT_NumericExt, ds_data, ds_weights, dim_out=idx_out) Out[30]: <pre>dim_from  dim_to\na         F          5.0\nb         D          7.0\n          F          3.0\nc         E         21.0\n          F          9.0\nName: 0, dtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"01-examples/#examples","title":"Examples\u00b6","text":""},{"location":"01-examples/#basic-transformation","title":"Basic transformation\u00b6","text":""},{"location":"01-examples/#basic-transformation-with-pandas","title":"Basic transformation with pandas\u00b6","text":""},{"location":"01-examples/#change-output-dimensions","title":"change output dimensions\u00b6","text":"<p>Sometimes we need to keep some of the origial index dimensions. In this case we we have to specify the desired output dimension index explicitly</p>"},{"location":"02-api/","title":"API","text":"<p>options: members: - transform_pandas - transform</p>"},{"location":"02-api/#data_disaggregation.vtypes","title":"<code>data_disaggregation.vtypes</code>","text":"<p>Type classes for data.</p>"},{"location":"02-api/#data_disaggregation.vtypes.VT_Nominal","title":"<code>VT_Nominal</code>","text":"<p>               Bases: <code>VariableType</code></p> <p>Type class for nominal (categorical) data.</p> <ul> <li>Aggregation method: mode (most commonly used)</li> <li>Disaggregation method: keep value</li> <li>Examples: regional codes</li> </ul>"},{"location":"02-api/#data_disaggregation.vtypes.VT_Numeric","title":"<code>VT_Numeric</code>","text":"<p>               Bases: <code>VariableType</code></p> <p>Type class for numerical, intensive data</p> <p>An intensive variable is one which does not scale with the system size.</p> <ul> <li>Aggregation method: weighted average</li> <li>Disaggregation method: keep value</li> <li>Examples: temperature, density, pressure</li> </ul>"},{"location":"02-api/#data_disaggregation.vtypes.VT_NumericExt","title":"<code>VT_NumericExt</code>","text":"<p>               Bases: <code>VT_Numeric</code></p> <p>Type class for numerical, extensive data.</p> <p>An extensive variable is one which does scale with the system size (assuming an equal distribution).</p> <ul> <li>Aggregation method: sum</li> <li>Disaggregation method: distribute by weights</li> <li>Examples: population, energy, total cost</li> </ul>"},{"location":"02-api/#data_disaggregation.vtypes.VT_Ordinal","title":"<code>VT_Ordinal</code>","text":"<p>               Bases: <code>VT_Nominal</code></p> <p>Type class for ordinal data (ranked categorical).</p> <ul> <li>Aggregation method: median</li> <li>Disaggregation method: keep value</li> <li>Examples: Level of agreement</li> </ul>"},{"location":"02-api/#data_disaggregation.vtypes.VariableType","title":"<code>VariableType</code>","text":"<p>               Bases: <code>ABC</code></p>"},{"location":"02-api/#data_disaggregation.vtypes.VariableType.weighted_aggregate","title":"<code>weighted_aggregate(data)</code>  <code>classmethod</code>","text":"<p>aggregate data</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Iterable</code> <p>non empty list of (value, weight) pairs. weights must be numerical, positive, and sum up to 1.0.</p> required <p>Returns:</p> Type Description <code>aggregated value</code>"},{"location":"02-api/#data_disaggregation.actions","title":"<code>data_disaggregation.actions</code>","text":"<p>Functions to perform data transformations.</p>"},{"location":"02-api/#data_disaggregation.actions.transform","title":"<code>transform(vtype, data, weight_map, weights_from=None, weights_to=None, weight_rel_threshold=0.0, validate=True)</code>","text":"<p>(dis-)aggregate data.</p> <p>Parameters:</p> Name Type Description Default <code>vtype</code> <code>VariableType</code> <p>Variable type of input data, determines the aggregation method.</p> required <code>data</code> <code>Mapping[F, V]</code> <p>Input data: mapping (usually dict) from any keys (any hashable) to values.</p> required <code>weight_map</code> <code>Mapping[Tuple[F, T], float]</code> <p>weights for combinations of input and output elements (must be positive). Keys must tuples from input/output key pairs.</p> required <code>weights_from</code> <code>Mapping[F, float]</code> <p>optional weights of input elements (must be positive). If not specified, this will be calculated as a sum from <code>weight_map</code>.</p> <code>None</code> <code>weights_to</code> <code>Mapping[T, float]</code> <p>optional weights of output elements (must be positive). If not specified, this will be calculated as a sum from <code>weight_map</code>.</p> <code>None</code> <code>weight_rel_threshold</code> <code>float</code> <p>optional value between 0 and 1: all mappings are dropped if the sum of input weights / output weight is smaller than this threshold. For example, you may want to set it to 0.5 for geographical mappings with extensive data.</p> <code>0.0</code> <code>validate</code> <code>bool</code> <p>if True: run additional (but costly) validations of weights and data.</p> <code>True</code> <p>Returns:</p> Type Description <code>Mapping[T, V]</code> <p>output data as a mapping from output keys (any hashable) to values.</p>"},{"location":"02-api/#data_disaggregation.actions.transform_pandas","title":"<code>transform_pandas(vtype, data, weights, dim_in=None, dim_out=None, validate=True)</code>","text":"<p>(dis-)aggregate data (pandas).</p> <p>Parameters:</p> Name Type Description Default <code>vtype</code> <code>VariableType</code> <p>Variable type of input data, determines the aggregation method.</p> required <code>data</code> <code>Union[DataFrame, Series, float]</code> <p>Input data. Indices must be unique and have unique level names</p> required <code>weights</code> <code>Union[Index, Series, Tuple[Union[Index, Series]]]</code> <p>weights for combinations of input and output elements (must be positive).</p> required <code>dim_in</code> <code>Union[Index, Series]</code> <p>TODO</p> <code>None</code> <code>dim_out</code> <code>Union[Index, Series]</code> <p>TODO</p> <code>None</code> <code>validate</code> <code>bool</code> <p>if True: run additional (but costly) validations of weights and data.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[DataFrame, Series, float]</code> <p>output data</p>"}]}